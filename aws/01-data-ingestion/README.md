# 01 - Data Ingestion (Batch) - AWS

This module contains examples of **batch** data ingestion on AWS, focusing on simple, scalable, and low-cost solutions â€” ideal for starting data pipelines aligned with DataOps best practices.

---

## ğŸ¯ Objective

Demonstrate how to perform batch ingestions in an automated, versioned, and observable way, using **AWS Lambda** to fetch data from external sources and store it in **Amazon S3**, organized by date.

---

## ğŸ§± Available Subprojects

| Subproject                    | Description |
|------------------------------|-------------|
| `lambda_ingest_api_to_s3/`   | Daily ingestion of data from a public API into S3, with date-based versioning |

---

## ğŸ§° Technologies and Services Used

- **AWS Lambda**  
- **Amazon S3**  
- **Amazon EventBridge** (scheduling)  
- **Terraform** (infrastructure as code)  
- **AWS CloudWatch** (logs and metrics)  

---

## ğŸ§ª Applied DataOps Principles

- âš™ï¸ Infrastructure as Code (IaC)  
- ğŸ” Versioned and reproducible pipelines  
- ğŸ“Š Observability with logs and dashboards  
- ğŸ§ª Basic tests to ensure ingestion success  
- ğŸ’° Simplicity and cost control (FinOps)  

---

## ğŸš€ How to Use

1. Navigate to the desired subproject:
```bash
cd lambda_ingest_api_to_s3
